{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5276f554",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9af77a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1) Scraping Data Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b78d97",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to c:\\users\\kirolos\\appdata\\local\\temp\\pip-req-build-e_wz1ezi\n",
      "  Resolved https://github.com/JustAnotherArchivist/snscrape.git to commit d72b51953f0ec05ee18761ea31c1bb82f886f7a9\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from snscrape==0.4.3.20220107.dev56+gd72b519) (2.27.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from snscrape==0.4.3.20220107.dev56+gd72b519) (4.8.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from snscrape==0.4.3.20220107.dev56+gd72b519) (4.11.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from snscrape==0.4.3.20220107.dev56+gd72b519) (3.7.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->snscrape==0.4.3.20220107.dev56+gd72b519) (2.3.2.post1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->snscrape==0.4.3.20220107.dev56+gd72b519) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->snscrape==0.4.3.20220107.dev56+gd72b519) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->snscrape==0.4.3.20220107.dev56+gd72b519) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->snscrape==0.4.3.20220107.dev56+gd72b519) (1.26.9)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->snscrape==0.4.3.20220107.dev56+gd72b519) (1.7.1)\n",
      "Building wheels for collected packages: snscrape\n",
      "  Building wheel for snscrape (setup.py): started\n",
      "  Building wheel for snscrape (setup.py): finished with status 'done'\n",
      "  Created wheel for snscrape: filename=snscrape-0.4.3.20220107.dev56+gd72b519-py3-none-any.whl size=68190 sha256=ff63d09143bf7e9e1aef6f7085e0b77f04a9069c9133ea3160e4d3786d0485f0\n",
      "  Stored in directory: C:\\Users\\Kirolos\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-zjwkjc_s\\wheels\\05\\e9\\f7\\57056e7c7e44b1feed932fa49fdec9d706c4f563e37160ab74\n",
      "Successfully built snscrape\n",
      "Installing collected packages: snscrape\n",
      "Successfully installed snscrape-0.4.3.20220107.dev56+gd72b519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/JustAnotherArchivist/snscrape.git 'C:\\Users\\Kirolos\\AppData\\Local\\Temp\\pip-req-build-e_wz1ezi'\n"
     ]
    }
   ],
   "source": [
    "# It requires python 3.8 or higher\n",
    "!pip3 install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5de8abf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It spent 3 hours for scraping more than 180,000 tweets\n",
    "text_query = \"$NFLX\"\n",
    "since_date = \"2018-01-01\"\n",
    "until_date = \"2022-07-11\"\n",
    "os.system('snscrape --jsonl --since {} twitter-search \"{} until:{}\"> text-query-tweets.json'.format(since_date, text_query, until_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db7269",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### a) Reading the output jason file by pandas as a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16b653b6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Reading jason files as dataframes\n",
    "tweets_df = pd.read_json('text-query-tweets.json', lines=True)\n",
    "# tweets_df2 = pd.read_json('text-query-tweets2.json', lines=True)\n",
    "# Merging both dataframes as a single dataframe\n",
    "# tweets_df = pd.concat([tweets_df1,tweets_df2],ignore_index=True)\n",
    "# Selecting the important columns only wich are Data,renderContent and Lang\n",
    "tweets_content = tweets_df.loc[:,['date','renderedContent','lang']]\n",
    "# Choosing the tweets in english language only\n",
    "tweets_content = tweets_content[tweets_content['lang']=='en']\n",
    "# Dropping the lang column\n",
    "tweets_content.drop(\"lang\",axis=1,inplace=True)\n",
    "# Download the CSV file result on the current folder.\n",
    "tweets_content.to_csv('Ntweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4883bb56",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2) NLP Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d46d0ea7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: demoji in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install demoji\n",
    "import demoji\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import pos_tag\n",
    "import attr\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ed251",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### a) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0078b7b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This function is used to pass the POS tage for each word passed through clean_text function\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "088177e9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning tweets\n",
    "def clean_text(text):\n",
    "    # Initialization the twitter tokenizer\n",
    "    tk = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True) \n",
    "    # Initialization the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()  \n",
    "    # Trying to avoid deleting the negative verbs as it affects the meaning of the tweets.\n",
    "    stop_words = stopwords.words('english') + [\"i'll\",\"i'm\", \"should\", \"could\"]\n",
    "    negative_verbs = [ \"shan't\",'shouldn',\"shouldn't\",'wasn','weren','won','wouldn','aren','couldn','didn','doesn','hadn','hasn','haven','isn','ma','mightn','mustn',\"mustn't\",'needn',\"needn't\",\"wouldn't\",\"won't\",\"weren't\",\"wasn't\",\"couldn\",\"not\",\"nor\",\"no\",\"mightn't\",\"isn't\",\"haven't\",\"hadn't\",\"hasn't\",\"didn't\",\"doesn't\",\"aren't\",\"don't\",\"couldn't\",\"never\"]\n",
    "    stop_words =[word for word in stop_words if word not in negative_verbs ] \n",
    "    \n",
    "    # Lowering tweets\n",
    "    lower_tweet = text.lower() \n",
    "    # Removing hashtag and cashtag symbols\n",
    "    tweet = re.sub(r\"[#$]\",\" \",lower_tweet)\n",
    "    # Removing links from tweets\n",
    "    tweet = re.sub(r\"https?:\\/\\/.*[\\r\\n]*\",\" \", tweet)\n",
    "    # Translating emojies into thier descriptions\n",
    "    tweet = demoji.replace_with_desc(tweet)\n",
    "    # removing numerical values\n",
    "    tweet = re.sub(r\"[0-9]|-->\",\"\",tweet)\n",
    "    # Tokenize the tweets by twitter tokenzier.\n",
    "    tweet = tk.tokenize(tweet)\n",
    "    # Choosing the words that don't exist in stopwords, thier lengths are more than 2 letters and then lemmatize them.\n",
    "    tweet = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tweet if word not in stop_words and word not in string.punctuation and len(word)>2 and \".\" not in word]\n",
    "    # return the tokens in one sentence \n",
    "    tweet = \" \".join(tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fcd647b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                        date                                                                                                                                                                                                                                                                              renderedContent\n0  2019-12-31 23:54:41+00:00                                                                                                                                                           Original content rather than licensed titles doninating $NFLX's 2020 viewership (Netflix data taken with a pinch of salt as ever).\n1  2019-12-31 23:43:26+00:00                                                                                                                                                                                                                                    $FB $AMZN $GOOGL $NFLX #FANG 2019 https://t.co/RdVlnrPiR3\n2  2019-12-31 23:13:37+00:00  i think many retail folks early next week will look back on this end of year tape paint in markets today as a missed opportunity to sell the longs they were waiting to sell due to 2021 tax deferrance. \\n\\nWe'll see.\\n\\n$SPX $SPY $TSLA $NFLX $AAPL $AMD $NVDA $VIX $VXX $VXXB $QQQ $IBB\n3  2019-12-31 23:01:00+00:00                                                                                                                #Netflix continues to grow its global subscriber base but it's about to face some stiff competition from #Apple and #Disney. #stocks $NFLX $DIS $AAPL https://t.co/sIJu4fzAl0\n4  2019-12-31 22:56:49+00:00                                                                                                                                                                  If only... $NFLX and chill? No, $NFLX &amp; $DPZ. Both started the decade at 7.93 and 8.53 respectively. nearly 4000% gain.",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>renderedContent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-12-31 23:54:41+00:00</td>\n      <td>Original content rather than licensed titles doninating $NFLX's 2020 viewership (Netflix data taken with a pinch of salt as ever).</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-12-31 23:43:26+00:00</td>\n      <td>$FB $AMZN $GOOGL $NFLX #FANG 2019 https://t.co/RdVlnrPiR3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019-12-31 23:13:37+00:00</td>\n      <td>i think many retail folks early next week will look back on this end of year tape paint in markets today as a missed opportunity to sell the longs they were waiting to sell due to 2021 tax deferrance. \\n\\nWe'll see.\\n\\n$SPX $SPY $TSLA $NFLX $AAPL $AMD $NVDA $VIX $VXX $VXXB $QQQ $IBB</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019-12-31 23:01:00+00:00</td>\n      <td>#Netflix continues to grow its global subscriber base but it's about to face some stiff competition from #Apple and #Disney. #stocks $NFLX $DIS $AAPL https://t.co/sIJu4fzAl0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-12-31 22:56:49+00:00</td>\n      <td>If only... $NFLX and chill? No, $NFLX &amp;amp; $DPZ. Both started the decade at 7.93 and 8.53 respectively. nearly 4000% gain.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the scrapped file\n",
    "tweets = pd.read_csv(\"Ntweets.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "899c409b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Applying text cleaning and then downloading it on the current folder\n",
    "tweets['cleaned'] = tweets[\"renderedContent\"].apply(lambda row:clean_text(row))\n",
    "tweets.to_csv(\"CleanedNTweets.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501df02d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### b) Sentiment analysis by pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "473c8cec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.19.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: transformers[sentencepiece] in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.19.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (0.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (2022.4.24)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (4.64.0)\n",
      "Requirement already satisfied: requests in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (0.12.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (3.7.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (3.19.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (0.1.96)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers[sentencepiece]) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[sentencepiece]) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[sentencepiece]) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[sentencepiece]) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kirolos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[sentencepiece]) (2.0.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "316d2142",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kirolos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "C:\\Users\\Kirolos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_masked\\__init__.py:223: UserWarning: Failed to initialize NumPy: module compiled against API version 0xf but this version of numpy is 0xe (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  example_input = torch.tensor([[-3, -2, -1], [0, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e6d8a6b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                        date                                                                                                                                                                                                                                                                              renderedContent                                                                                                                                                                                                    cleaned\n0  2019-12-31 23:54:41+00:00                                                                                                                                                           Original content rather than licensed titles doninating $NFLX's 2020 viewership (Netflix data taken with a pinch of salt as ever).                                                                                                       original content rather license title doninating nflx's viewership netflix data take pinch salt ever\n1  2019-12-31 23:43:26+00:00                                                                                                                                                                                                                                    $FB $AMZN $GOOGL $NFLX #FANG 2019 https://t.co/RdVlnrPiR3                                                                                                                                                                                       amzn googl nflx fang\n2  2019-12-31 23:13:37+00:00  i think many retail folks early next week will look back on this end of year tape paint in markets today as a missed opportunity to sell the longs they were waiting to sell due to 2021 tax deferrance. \\n\\nWe'll see.\\n\\n$SPX $SPY $TSLA $NFLX $AAPL $AMD $NVDA $VIX $VXX $VXXB $QQQ $IBB  think many retail folk early next week look back end year tape paint market today miss opportunity sell longs wait sell due tax deferrance we'll see spx spy tsla nflx aapl amd nvda vix vxx vxxb qqq ibb\n3  2019-12-31 23:01:00+00:00                                                                                                                #Netflix continues to grow its global subscriber base but it's about to face some stiff competition from #Apple and #Disney. #stocks $NFLX $DIS $AAPL https://t.co/sIJu4fzAl0                                                                                                       netflix continue grow global subscriber base face stiff competition apple disney stock nflx dis aapl\n4  2019-12-31 22:56:49+00:00                                                                                                                                                                  If only... $NFLX and chill? No, $NFLX &amp; $DPZ. Both started the decade at 7.93 and 8.53 respectively. nearly 4000% gain.                                                                                                                                                  nflx chill nflx dpz start decade respectively nearly gain",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>renderedContent</th>\n      <th>cleaned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-12-31 23:54:41+00:00</td>\n      <td>Original content rather than licensed titles doninating $NFLX's 2020 viewership (Netflix data taken with a pinch of salt as ever).</td>\n      <td>original content rather license title doninating nflx's viewership netflix data take pinch salt ever</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-12-31 23:43:26+00:00</td>\n      <td>$FB $AMZN $GOOGL $NFLX #FANG 2019 https://t.co/RdVlnrPiR3</td>\n      <td>amzn googl nflx fang</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019-12-31 23:13:37+00:00</td>\n      <td>i think many retail folks early next week will look back on this end of year tape paint in markets today as a missed opportunity to sell the longs they were waiting to sell due to 2021 tax deferrance. \\n\\nWe'll see.\\n\\n$SPX $SPY $TSLA $NFLX $AAPL $AMD $NVDA $VIX $VXX $VXXB $QQQ $IBB</td>\n      <td>think many retail folk early next week look back end year tape paint market today miss opportunity sell longs wait sell due tax deferrance we'll see spx spy tsla nflx aapl amd nvda vix vxx vxxb qqq ibb</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019-12-31 23:01:00+00:00</td>\n      <td>#Netflix continues to grow its global subscriber base but it's about to face some stiff competition from #Apple and #Disney. #stocks $NFLX $DIS $AAPL https://t.co/sIJu4fzAl0</td>\n      <td>netflix continue grow global subscriber base face stiff competition apple disney stock nflx dis aapl</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-12-31 22:56:49+00:00</td>\n      <td>If only... $NFLX and chill? No, $NFLX &amp;amp; $DPZ. Both started the decade at 7.93 and 8.53 respectively. nearly 4000% gain.</td>\n      <td>nflx chill nflx dpz start decade respectively nearly gain</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"CleanedNTweets.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "162a40e7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49b754e4846b421394b26576913537fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFXLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "('cardiffnlp/twitter-xlm-roberta-base-sentiment\\\\tokenizer_config.json',\n 'cardiffnlp/twitter-xlm-roberta-base-sentiment\\\\special_tokens_map.json',\n 'cardiffnlp/twitter-xlm-roberta-base-sentiment\\\\sentencepiece.bpe.model',\n 'cardiffnlp/twitter-xlm-roberta-base-sentiment\\\\added_tokens.json',\n 'cardiffnlp/twitter-xlm-roberta-base-sentiment\\\\tokenizer.json')"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "\n",
    "# TF\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)\n",
    "tokenizer.save_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41b3070a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def polarity(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='tf')\n",
    "    output = model(encoded_input)\n",
    "    scores = output[0][0].numpy()\n",
    "    scores = softmax(scores)\n",
    "    \n",
    "    # Print labels and scores\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    l = config.id2label[ranking[0]]\n",
    "    plrty = -1 if l == \"Negative\" else 1 if l == \"Positive\" else 0 \n",
    "    s = np.round(float(scores[ranking[0]]), 4)\n",
    "    return (l,plrty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f1aa3a6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# downloading the file after applying sentiment analysis on the current folder\n",
    "tweets['label'],tweets['Polarity'] = zip(*tweets['cleaned'].apply(lambda txt:polarity(txt)))\n",
    "tweets.to_csv(\"polarizedTweets.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6ce6e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3) Preparing Data for time series model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8176302d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                        date                                                                                                                                                                                                                                                                              renderedContent                                                                                                                                                                                                    cleaned     label  Polarity\n0  2019-12-31 23:54:41+00:00                                                                                                                                                           Original content rather than licensed titles doninating $NFLX's 2020 viewership (Netflix data taken with a pinch of salt as ever).                                                                                                       original content rather license title doninating nflx's viewership netflix data take pinch salt ever  Negative        -1\n1  2019-12-31 23:43:26+00:00                                                                                                                                                                                                                                    $FB $AMZN $GOOGL $NFLX #FANG 2019 https://t.co/RdVlnrPiR3                                                                                                                                                                                       amzn googl nflx fang   Neutral         0\n2  2019-12-31 23:13:37+00:00  i think many retail folks early next week will look back on this end of year tape paint in markets today as a missed opportunity to sell the longs they were waiting to sell due to 2021 tax deferrance. \\n\\nWe'll see.\\n\\n$SPX $SPY $TSLA $NFLX $AAPL $AMD $NVDA $VIX $VXX $VXXB $QQQ $IBB  think many retail folk early next week look back end year tape paint market today miss opportunity sell longs wait sell due tax deferrance we'll see spx spy tsla nflx aapl amd nvda vix vxx vxxb qqq ibb   Neutral         0\n3  2019-12-31 23:01:00+00:00                                                                                                                #Netflix continues to grow its global subscriber base but it's about to face some stiff competition from #Apple and #Disney. #stocks $NFLX $DIS $AAPL https://t.co/sIJu4fzAl0                                                                                                       netflix continue grow global subscriber base face stiff competition apple disney stock nflx dis aapl   Neutral         0\n4  2019-12-31 22:56:49+00:00                                                                                                                                                                  If only... $NFLX and chill? No, $NFLX &amp; $DPZ. Both started the decade at 7.93 and 8.53 respectively. nearly 4000% gain.                                                                                                                                                  nflx chill nflx dpz start decade respectively nearly gain   Neutral         0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>renderedContent</th>\n      <th>cleaned</th>\n      <th>label</th>\n      <th>Polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-12-31 23:54:41+00:00</td>\n      <td>Original content rather than licensed titles doninating $NFLX's 2020 viewership (Netflix data taken with a pinch of salt as ever).</td>\n      <td>original content rather license title doninating nflx's viewership netflix data take pinch salt ever</td>\n      <td>Negative</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-12-31 23:43:26+00:00</td>\n      <td>$FB $AMZN $GOOGL $NFLX #FANG 2019 https://t.co/RdVlnrPiR3</td>\n      <td>amzn googl nflx fang</td>\n      <td>Neutral</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019-12-31 23:13:37+00:00</td>\n      <td>i think many retail folks early next week will look back on this end of year tape paint in markets today as a missed opportunity to sell the longs they were waiting to sell due to 2021 tax deferrance. \\n\\nWe'll see.\\n\\n$SPX $SPY $TSLA $NFLX $AAPL $AMD $NVDA $VIX $VXX $VXXB $QQQ $IBB</td>\n      <td>think many retail folk early next week look back end year tape paint market today miss opportunity sell longs wait sell due tax deferrance we'll see spx spy tsla nflx aapl amd nvda vix vxx vxxb qqq ibb</td>\n      <td>Neutral</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019-12-31 23:01:00+00:00</td>\n      <td>#Netflix continues to grow its global subscriber base but it's about to face some stiff competition from #Apple and #Disney. #stocks $NFLX $DIS $AAPL https://t.co/sIJu4fzAl0</td>\n      <td>netflix continue grow global subscriber base face stiff competition apple disney stock nflx dis aapl</td>\n      <td>Neutral</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-12-31 22:56:49+00:00</td>\n      <td>If only... $NFLX and chill? No, $NFLX &amp;amp; $DPZ. Both started the decade at 7.93 and 8.53 respectively. nearly 4000% gain.</td>\n      <td>nflx chill nflx dpz start decade respectively nearly gain</td>\n      <td>Neutral</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptweets = pd.read_csv(\"polarizedTweets.csv\")\n",
    "ptweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cdd7206",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                        date  Polarity\n0  2019-12-31 23:54:41+00:00        -1\n1  2019-12-31 23:43:26+00:00         0\n2  2019-12-31 23:13:37+00:00         0\n3  2019-12-31 23:01:00+00:00         0\n4  2019-12-31 22:56:49+00:00         0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>Polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-12-31 23:54:41+00:00</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-12-31 23:43:26+00:00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019-12-31 23:13:37+00:00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019-12-31 23:01:00+00:00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-12-31 22:56:49+00:00</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the date and polarized values from the previous dataframe\n",
    "ptweets_df = ptweets.loc[:,[\"date\",\"Polarity\"]]\n",
    "ptweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "077ee074",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              P_mean  P_sum  twt_count\ndate                                  \n2018-01-01  0.007519      1        133\n2018-01-02  0.020833     10        480\n2018-01-03  0.071217     24        337\n2018-01-04 -0.018519     -4        216\n2018-01-05 -0.019737     -6        304",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>P_mean</th>\n      <th>P_sum</th>\n      <th>twt_count</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-01-01</th>\n      <td>0.007519</td>\n      <td>1</td>\n      <td>133</td>\n    </tr>\n    <tr>\n      <th>2018-01-02</th>\n      <td>0.020833</td>\n      <td>10</td>\n      <td>480</td>\n    </tr>\n    <tr>\n      <th>2018-01-03</th>\n      <td>0.071217</td>\n      <td>24</td>\n      <td>337</td>\n    </tr>\n    <tr>\n      <th>2018-01-04</th>\n      <td>-0.018519</td>\n      <td>-4</td>\n      <td>216</td>\n    </tr>\n    <tr>\n      <th>2018-01-05</th>\n      <td>-0.019737</td>\n      <td>-6</td>\n      <td>304</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the date format to match with the next csv file date format\n",
    "ptweets_df['date'] =pd.to_datetime(ptweets_df['date'],infer_datetime_format=True)\n",
    "ptweets_df['date'] =pd.to_datetime(ptweets_df['date'].dt.strftime(\"%m/%d/%y\"))\n",
    "\n",
    "# Aggregate the tweets polarization by avergae, sum and counts \n",
    "Pol_df = pd.DataFrame(ptweets_df.groupby('date')['Polarity'].mean())\n",
    "Pol_df.rename(columns={\"Polarity\":\"P_mean\"},inplace=True)\n",
    "Pol_df['P_sum'] = ptweets_df.groupby('date')['Polarity'].sum()\n",
    "Pol_df['twt_count'] = ptweets_df.groupby('date')['Polarity'].count()\n",
    "Pol_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e13c383",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        date        Open        High         Low       Close   Adj Close    Volume    P_mean  P_sum  twt_count\n0 2018-01-02  196.100006  201.649994  195.419998  201.070007  201.070007  10966900  0.020833     10        480\n1 2018-01-03  202.050003  206.210007  201.500000  205.050003  205.050003   8591400  0.071217     24        337\n2 2018-01-04  206.199997  207.050003  204.000000  205.630005  205.630005   6029600 -0.018519     -4        216\n3 2018-01-05  207.250000  210.020004  205.589996  209.990005  209.990005   7033200 -0.019737     -6        304\n4 2018-01-08  210.020004  212.500000  208.440002  212.050003  212.050003   5580200 -0.007663     -2        261",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n      <th>P_mean</th>\n      <th>P_sum</th>\n      <th>twt_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-01-02</td>\n      <td>196.100006</td>\n      <td>201.649994</td>\n      <td>195.419998</td>\n      <td>201.070007</td>\n      <td>201.070007</td>\n      <td>10966900</td>\n      <td>0.020833</td>\n      <td>10</td>\n      <td>480</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-01-03</td>\n      <td>202.050003</td>\n      <td>206.210007</td>\n      <td>201.500000</td>\n      <td>205.050003</td>\n      <td>205.050003</td>\n      <td>8591400</td>\n      <td>0.071217</td>\n      <td>24</td>\n      <td>337</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-01-04</td>\n      <td>206.199997</td>\n      <td>207.050003</td>\n      <td>204.000000</td>\n      <td>205.630005</td>\n      <td>205.630005</td>\n      <td>6029600</td>\n      <td>-0.018519</td>\n      <td>-4</td>\n      <td>216</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-01-05</td>\n      <td>207.250000</td>\n      <td>210.020004</td>\n      <td>205.589996</td>\n      <td>209.990005</td>\n      <td>209.990005</td>\n      <td>7033200</td>\n      <td>-0.019737</td>\n      <td>-6</td>\n      <td>304</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-01-08</td>\n      <td>210.020004</td>\n      <td>212.500000</td>\n      <td>208.440002</td>\n      <td>212.050003</td>\n      <td>212.050003</td>\n      <td>5580200</td>\n      <td>-0.007663</td>\n      <td>-2</td>\n      <td>261</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the netflix finance data and preparing it to fit with the polarized values\n",
    "nflx_df = pd.read_csv(\"NFLX.csv\")\n",
    "nflx_df.rename(columns={\"Date\":\"date\"},inplace=True)\n",
    "nflx_df['date'] = pd.to_datetime(nflx_df['date'],infer_datetime_format=True)\n",
    "nflx_df.set_index(\"date\")\n",
    "# Adding the polarization column in the netflix dataframe.\n",
    "final_df = nflx_df.join(Pol_df,on='date',how=\"inner\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Downloading the final CSV file that has the finance data and tweets polarizations\n",
    "final_df.to_csv(\"FinalNflx.csv\",index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "            date        Open        High         Low       Close   Adj Close    Volume    P_mean  P_sum  twt_count\n0     2018-01-02  196.100006  201.649994  195.419998  201.070007  201.070007  10966900  0.020833     10        480\n1     2018-01-03  202.050003  206.210007  201.500000  205.050003  205.050003   8591400  0.071217     24        337\n2     2018-01-04  206.199997  207.050003  204.000000  205.630005  205.630005   6029600 -0.018519     -4        216\n3     2018-01-05  207.250000  210.020004  205.589996  209.990005  209.990005   7033200 -0.019737     -6        304\n4     2018-01-08  210.020004  212.500000  208.440002  212.050003  212.050003   5580200 -0.007663     -2        261\n...          ...         ...         ...         ...         ...         ...       ...       ...    ...        ...\n1132  2022-07-01  176.490005  180.100006  174.270004  179.949997  179.949997   5194700 -0.062315    -21        337\n1133  2022-07-05  176.279999  185.919998  172.679993  185.880005  185.880005   7334300 -0.058824    -25        425\n1134  2022-07-06  185.199997  186.220001  180.820007  184.059998  184.059998   5753400 -0.014870     -8        538\n1135  2022-07-07  184.270004  190.210007  183.500000  189.270004  189.270004   6334500 -0.055427    -24        433\n1136  2022-07-08  186.020004  189.910004  182.750000  186.979996  186.979996   5831300 -0.043011    -12        279\n\n[1137 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n      <th>P_mean</th>\n      <th>P_sum</th>\n      <th>twt_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-01-02</td>\n      <td>196.100006</td>\n      <td>201.649994</td>\n      <td>195.419998</td>\n      <td>201.070007</td>\n      <td>201.070007</td>\n      <td>10966900</td>\n      <td>0.020833</td>\n      <td>10</td>\n      <td>480</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-01-03</td>\n      <td>202.050003</td>\n      <td>206.210007</td>\n      <td>201.500000</td>\n      <td>205.050003</td>\n      <td>205.050003</td>\n      <td>8591400</td>\n      <td>0.071217</td>\n      <td>24</td>\n      <td>337</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-01-04</td>\n      <td>206.199997</td>\n      <td>207.050003</td>\n      <td>204.000000</td>\n      <td>205.630005</td>\n      <td>205.630005</td>\n      <td>6029600</td>\n      <td>-0.018519</td>\n      <td>-4</td>\n      <td>216</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-01-05</td>\n      <td>207.250000</td>\n      <td>210.020004</td>\n      <td>205.589996</td>\n      <td>209.990005</td>\n      <td>209.990005</td>\n      <td>7033200</td>\n      <td>-0.019737</td>\n      <td>-6</td>\n      <td>304</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-01-08</td>\n      <td>210.020004</td>\n      <td>212.500000</td>\n      <td>208.440002</td>\n      <td>212.050003</td>\n      <td>212.050003</td>\n      <td>5580200</td>\n      <td>-0.007663</td>\n      <td>-2</td>\n      <td>261</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1132</th>\n      <td>2022-07-01</td>\n      <td>176.490005</td>\n      <td>180.100006</td>\n      <td>174.270004</td>\n      <td>179.949997</td>\n      <td>179.949997</td>\n      <td>5194700</td>\n      <td>-0.062315</td>\n      <td>-21</td>\n      <td>337</td>\n    </tr>\n    <tr>\n      <th>1133</th>\n      <td>2022-07-05</td>\n      <td>176.279999</td>\n      <td>185.919998</td>\n      <td>172.679993</td>\n      <td>185.880005</td>\n      <td>185.880005</td>\n      <td>7334300</td>\n      <td>-0.058824</td>\n      <td>-25</td>\n      <td>425</td>\n    </tr>\n    <tr>\n      <th>1134</th>\n      <td>2022-07-06</td>\n      <td>185.199997</td>\n      <td>186.220001</td>\n      <td>180.820007</td>\n      <td>184.059998</td>\n      <td>184.059998</td>\n      <td>5753400</td>\n      <td>-0.014870</td>\n      <td>-8</td>\n      <td>538</td>\n    </tr>\n    <tr>\n      <th>1135</th>\n      <td>2022-07-07</td>\n      <td>184.270004</td>\n      <td>190.210007</td>\n      <td>183.500000</td>\n      <td>189.270004</td>\n      <td>189.270004</td>\n      <td>6334500</td>\n      <td>-0.055427</td>\n      <td>-24</td>\n      <td>433</td>\n    </tr>\n    <tr>\n      <th>1136</th>\n      <td>2022-07-08</td>\n      <td>186.020004</td>\n      <td>189.910004</td>\n      <td>182.750000</td>\n      <td>186.979996</td>\n      <td>186.979996</td>\n      <td>5831300</td>\n      <td>-0.043011</td>\n      <td>-12</td>\n      <td>279</td>\n    </tr>\n  </tbody>\n</table>\n<p>1137 rows  10 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2018=pd.read_csv('nflx2018-2020.csv')\n",
    "df_2020=pd.read_csv('nflx2020-2022.csv')\n",
    "df_all=pd.concat([df_2018,df_2020])\n",
    "df_all=df_all.reset_index()\n",
    "df_all.drop('index',inplace=True,axis=1)\n",
    "df_all"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "df_all.to_csv('Final_nflx_data_2018-2022',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                             date  \\\n262788  2018-01-01 00:20:07+00:00   \n262789  2018-01-01 00:15:03+00:00   \n262790  2018-01-01 00:13:43+00:00   \n262791  2018-01-01 00:08:10+00:00   \n262792  2018-01-01 00:00:06+00:00   \n\n                                          renderedContent  \\\n262788  Updates on stocks from specstocks at Speculati...   \n262789  REAL TIME TRADE ALERTS via PRIVATE $TWTR FEED,...   \n262790  $nflx New Chappelle stand-up specials on Netfl...   \n262791  Are You Bullish Or Bearish On #Netflix? Start ...   \n262792  LEVERAGE VOLATILITY! EXPLORE STRATEGIES IN OUR...   \n\n                                                  cleaned  \n262788  update stock specstocks speculatingstocks puls...  \n262789  real time trade alert via private twtr feed pe...  \n262790  nflx new chappelle stand-up special netflix today  \n262791  bullish bearish netflix start trading nflx bit...  \n262792  leverage volatility explore strategy research ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>renderedContent</th>\n      <th>cleaned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>262788</th>\n      <td>2018-01-01 00:20:07+00:00</td>\n      <td>Updates on stocks from specstocks at Speculati...</td>\n      <td>update stock specstocks speculatingstocks puls...</td>\n    </tr>\n    <tr>\n      <th>262789</th>\n      <td>2018-01-01 00:15:03+00:00</td>\n      <td>REAL TIME TRADE ALERTS via PRIVATE $TWTR FEED,...</td>\n      <td>real time trade alert via private twtr feed pe...</td>\n    </tr>\n    <tr>\n      <th>262790</th>\n      <td>2018-01-01 00:13:43+00:00</td>\n      <td>$nflx New Chappelle stand-up specials on Netfl...</td>\n      <td>nflx new chappelle stand-up special netflix today</td>\n    </tr>\n    <tr>\n      <th>262791</th>\n      <td>2018-01-01 00:08:10+00:00</td>\n      <td>Are You Bullish Or Bearish On #Netflix? Start ...</td>\n      <td>bullish bearish netflix start trading nflx bit...</td>\n    </tr>\n    <tr>\n      <th>262792</th>\n      <td>2018-01-01 00:00:06+00:00</td>\n      <td>LEVERAGE VOLATILITY! EXPLORE STRATEGIES IN OUR...</td>\n      <td>leverage volatility explore strategy research ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sss=pd.read_csv('CleanedNTweets.csv')\n",
    "sss.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "         Date        Open        High         Low       Close   Adj Close  \\\n0  2018-01-02  196.100006  201.649994  195.419998  201.070007  201.070007   \n1  2018-01-03  202.050003  206.210007  201.500000  205.050003  205.050003   \n2  2018-01-04  206.199997  207.050003  204.000000  205.630005  205.630005   \n3  2018-01-05  207.250000  210.020004  205.589996  209.990005  209.990005   \n4  2018-01-08  210.020004  212.500000  208.440002  212.050003  212.050003   \n\n     Volume  \n0  10966900  \n1   8591400  \n2   6029600  \n3   7033200  \n4   5580200  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-01-02</td>\n      <td>196.100006</td>\n      <td>201.649994</td>\n      <td>195.419998</td>\n      <td>201.070007</td>\n      <td>201.070007</td>\n      <td>10966900</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-01-03</td>\n      <td>202.050003</td>\n      <td>206.210007</td>\n      <td>201.500000</td>\n      <td>205.050003</td>\n      <td>205.050003</td>\n      <td>8591400</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-01-04</td>\n      <td>206.199997</td>\n      <td>207.050003</td>\n      <td>204.000000</td>\n      <td>205.630005</td>\n      <td>205.630005</td>\n      <td>6029600</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-01-05</td>\n      <td>207.250000</td>\n      <td>210.020004</td>\n      <td>205.589996</td>\n      <td>209.990005</td>\n      <td>209.990005</td>\n      <td>7033200</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-01-08</td>\n      <td>210.020004</td>\n      <td>212.500000</td>\n      <td>208.440002</td>\n      <td>212.050003</td>\n      <td>212.050003</td>\n      <td>5580200</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sss=pd.read_csv('NFLX.csv')\n",
    "sss.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "         date        Open        High         Low       Close   Adj Close  \\\n0  2018-01-02  196.100006  201.649994  195.419998  201.070007  201.070007   \n1  2018-01-03  202.050003  206.210007  201.500000  205.050003  205.050003   \n2  2018-01-04  206.199997  207.050003  204.000000  205.630005  205.630005   \n3  2018-01-05  207.250000  210.020004  205.589996  209.990005  209.990005   \n4  2018-01-08  210.020004  212.500000  208.440002  212.050003  212.050003   \n\n     Volume    P_mean  P_sum  twt_count  \n0  10966900  0.020833     10        480  \n1   8591400  0.071217     24        337  \n2   6029600 -0.018519     -4        216  \n3   7033200 -0.019737     -6        304  \n4   5580200 -0.007663     -2        261  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n      <th>P_mean</th>\n      <th>P_sum</th>\n      <th>twt_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-01-02</td>\n      <td>196.100006</td>\n      <td>201.649994</td>\n      <td>195.419998</td>\n      <td>201.070007</td>\n      <td>201.070007</td>\n      <td>10966900</td>\n      <td>0.020833</td>\n      <td>10</td>\n      <td>480</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-01-03</td>\n      <td>202.050003</td>\n      <td>206.210007</td>\n      <td>201.500000</td>\n      <td>205.050003</td>\n      <td>205.050003</td>\n      <td>8591400</td>\n      <td>0.071217</td>\n      <td>24</td>\n      <td>337</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-01-04</td>\n      <td>206.199997</td>\n      <td>207.050003</td>\n      <td>204.000000</td>\n      <td>205.630005</td>\n      <td>205.630005</td>\n      <td>6029600</td>\n      <td>-0.018519</td>\n      <td>-4</td>\n      <td>216</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-01-05</td>\n      <td>207.250000</td>\n      <td>210.020004</td>\n      <td>205.589996</td>\n      <td>209.990005</td>\n      <td>209.990005</td>\n      <td>7033200</td>\n      <td>-0.019737</td>\n      <td>-6</td>\n      <td>304</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-01-08</td>\n      <td>210.020004</td>\n      <td>212.500000</td>\n      <td>208.440002</td>\n      <td>212.050003</td>\n      <td>212.050003</td>\n      <td>5580200</td>\n      <td>-0.007663</td>\n      <td>-2</td>\n      <td>261</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sss=pd.read_csv('Final_nflx_data_2018-2022')\n",
    "sss.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}